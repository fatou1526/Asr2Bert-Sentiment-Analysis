{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ASR to Bert Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"# Installation des bibliothèques nécessaires\n!pip install torch torchaudio transformers huggingface_hub\n!pip install speechbrain\n!pip install tqdm\n!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2023-10-16T09:21:46.210053Z","iopub.execute_input":"2023-10-16T09:21:46.210288Z","iopub.status.idle":"2023-10-16T09:22:21.835560Z","shell.execute_reply.started":"2023-10-16T09:21:46.210265Z","shell.execute_reply":"2023-10-16T09:22:21.834307Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.0.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2023.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nCollecting speechbrain\n  Downloading speechbrain-0.5.15-py3-none-any.whl (553 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.8/553.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting hyperpyyaml (from speechbrain)\n  Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from speechbrain) (1.3.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from speechbrain) (1.23.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from speechbrain) (21.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from speechbrain) (1.11.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from speechbrain) (0.1.99)\nRequirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from speechbrain) (2.0.0)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (from speechbrain) (2.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from speechbrain) (4.66.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from speechbrain) (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->speechbrain) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->speechbrain) (2023.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->speechbrain) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->speechbrain) (6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->speechbrain) (3.0.9)\nRequirement already satisfied: ruamel.yaml>=0.17.28 in /opt/conda/lib/python3.10/site-packages (from hyperpyyaml->speechbrain) (0.17.32)\nRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /opt/conda/lib/python3.10/site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->speechbrain) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->speechbrain) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->speechbrain) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->speechbrain) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->speechbrain) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\nInstalling collected packages: hyperpyyaml, speechbrain\nSuccessfully installed hyperpyyaml-1.2.2 speechbrain-0.5.15\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.9)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Connexion sur huggingface hub\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-10-16T09:22:53.857756Z","iopub.execute_input":"2023-10-16T09:22:53.858101Z","iopub.status.idle":"2023-10-16T09:22:54.095876Z","shell.execute_reply.started":"2023-10-16T09:22:53.858072Z","shell.execute_reply":"2023-10-16T09:22:54.094912Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdc7dee0f8c44b1ebbc3dd77e075c81a"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Première Partie\n### Mise en place du modèle ASR","metadata":{}},{"cell_type":"code","source":"import torch\nimport librosa\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\n# Chargement du modèle\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-french\"\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Pretraitement de l'audio\naudiopath = \"/kaggle/input/voices/common_voice_fr_33153455.wav\"\nspeech_array, sampling_rate = librosa.load(audiopath, sr=16_000)\ninputs = processor(speech_array, sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n# inference\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\ntranscription = processor.batch_decode(predicted_ids)\nprint(f\"La transcription de l'audio est {transcription}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T12:03:22.994279Z","iopub.execute_input":"2023-10-16T12:03:22.994966Z","iopub.status.idle":"2023-10-16T12:03:30.193291Z","shell.execute_reply.started":"2023-10-16T12:03:22.994931Z","shell.execute_reply":"2023-10-16T12:03:30.192293Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"La transcription de l'audio est ['blessé pendant la guerre il est soigné à saumur puis à paris']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Deuxieme partie\n### Ici, il s'agit de faire le finetuning d'un modèle Bert avec les données français de allocine movies review afin de construire un modèle permettant d'analyser le sentiment.\n### L'inférence (ASR et Bert) s'est faite avec un demo sur fastAPI","metadata":{}},{"cell_type":"code","source":"# Import les bibliothèques et modules nécessaires pour finetuner le modèle de Bert\nimport os\nimport torch\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertModel, AutoTokenizer, BertConfig\nimport huggingface_hub\nfrom huggingface_hub import PyTorchModelHubMixin\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\nimport wandb   # pour monitorer l'entrainement\nimport huggingface_hub\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T09:35:19.197311Z","iopub.execute_input":"2023-10-16T09:35:19.198210Z","iopub.status.idle":"2023-10-16T09:35:19.203397Z","shell.execute_reply.started":"2023-10-16T09:35:19.198179Z","shell.execute_reply":"2023-10-16T09:35:19.202316Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Definir le fichier de configuration utilisé pour le modèle\nconfig = {\n    \"model_name\": \"nlptown/bert-base-multilingual-uncased-sentiment\",\n    \"max_length\": 80,\n    \"trainfile\": \"/kaggle/input/allocine-movies-review/train.csv\",\n    \"testfile\": \"/kaggle/input/allocine-movies-review/test.csv\",\n    \"valfile\": \"/kaggle/input/allocine-movies-review/valid.csv\",\n    \"batch_size\": 10,\n    \"learning_rate\": 2e-5,\n    \"n_epochs\": 4,\n    \"n_classes\": 1,\n    \"device\": torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n\n}\n\n# Définir la classe pour le charger et l'indexation des données\nclass MyDataset(Dataset):\n    def __init__(self, csvfile, tokenizer_name, max_length):\n        self.df = pd.read_csv(csvfile)\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        text = self.df['review'][index]\n        label = self.df['polarity'][index]\n\n        inputs = self.tokenizer(text=text, max_length = self.max_length, padding = 'max_length', truncation =True, return_tensors='pt')\n\n        return {\n            'input_ids': inputs['input_ids'],\n            'attention_mask': inputs['attention_mask'],\n            'label': torch.tensor(label)\n\n        }\n\n# Method permettant de charger les données par batch\ndef dataloader(dataset, batch_size, shuffle):\n\n    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n\n# Definir la classe du modèle\nclass SentimentAnalysisBertModel(nn.Module, PyTorchModelHubMixin):\n    def __init__(self):\n        super(SentimentAnalysisBertModel, self).__init__()\n        self.pretrained_model = BertModel.from_pretrained(config['model_name'])   # bert base 768 hidden state\n        self.classifier = nn.Linear(768, config['n_classes'])  # MLP\n\n    def forward(self, input_ids, attention_mask):\n\n        output = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask)    # batch de 768\n        output = self.classifier(output.last_hidden_state)\n\n        return output\n\n# Method pour l'entrainement des données\ndef train_step(model, train_loader, optimizer, loss_fn, device):\n    model.train()\n    total_loss = 0\n\n    for data in tqdm(train_loader, total = len(train_loader)):\n        input_ids = data['input_ids'].squeeze(1).to(device)\n        attention_mask = data['attention_mask'].to(device)\n        label = data['label'].to(device)\n\n        optimizer.zero_grad()\n\n        output = model(input_ids, attention_mask)\n\n        loss = loss_fn(output, label.unsqueeze(1))\n\n        loss.backward()\n\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss/len(train_loader)\n\n# Method pour la validation les données\ndef validation_step(model, validation_loader, loss_fn, device):\n\n    total_loss = 0\n    correct_prediction = 0\n\n    with torch.no_grad():\n        for data in tqdm(validation_loader, total=len(validation_loader)):\n            input_ids = data['input_ids'].squeeze(1).to(device)\n            attention_mask = data['attention_mask'].to(device)\n            label = data['label'].to(device)\n\n            output = model(input_ids, attention_mask)\n\n            loss = loss_fn(output, label.unsqueeze(1))\n\n            pred = torch.max(torch.softmax(output, dim=1), dim=1)\n\n            total_loss += loss.item()\n\n            correct_prediction += torch.sum(pred.indices==label)\n\n    return total_loss/len(validation_loader), correct_prediction/len(validation_loader)\n\n# Method pour faire le test du mod_le avec les données de test\ndef test_step(model, test_loader, loss_fn, device):\n    model.eval()\n    total_loss = 0\n    correct_prediction = 0\n\n    with torch.no_grad():\n        for data in tqdm(test_loader, total=len(test_loader)):\n            input_ids = data['input_ids'].squeeze(1).to(device)\n            attention_mask = data['attention_mask'].to(device)\n            label = data['label'].to(device)\n\n            output = model(input_ids, attention_mask)\n\n            loss = loss_fn(output, label.unsqueeze(1))\n\n            pred = torch.max(torch.softmax(output, dim=1), dim=1)\n\n            total_loss += loss.item()\n\n            correct_prediction += torch.sum(pred.indices == label)\n\n    return total_loss / len(test_loader), correct_prediction / len(test_loader)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T09:36:43.453228Z","iopub.execute_input":"2023-10-16T09:36:43.453585Z","iopub.status.idle":"2023-10-16T09:36:43.469588Z","shell.execute_reply.started":"2023-10-16T09:36:43.453555Z","shell.execute_reply":"2023-10-16T09:36:43.468424Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def main():\n\n    tokenizer = AutoTokenizer.from_pretrained(config[\"model_name\"])\n\n    wandb.init(project=\"asr-bert-sentiment-analysis\")\n\n    train_dataset = MyDataset(config['trainfile'], config['model_name'], config['max_length'])\n    \n    test_dataset = MyDataset(config['testfile'], config['model_name'], config['max_length'])\n    \n    val_dataset = MyDataset(config['valfile'], config['model_name'], config['max_length'])\n\n    train_loader = dataloader(train_dataset, config['batch_size'], shuffle = True)\n\n    validation_loader = dataloader(val_dataset, config['batch_size'], shuffle = False)\n\n    test_loader = dataloader(test_dataset, config['batch_size'], shuffle=False)\n    \n    train_data = next(iter(train_loader))\n\n    model = SentimentAnalysisBertModel()\n\n    model.to(config['device'])\n\n    #output = model(data['input_ids'].squeeze(1), data['attention_mask'])\n\n    optimizer = AdamW(model.parameters(), lr = config['learning_rate'])\n\n    loss_fn = nn.CrossEntropyLoss()\n\n    for epoch in range(config['n_epochs']):\n        loss_train = train_step(model, train_loader, optimizer, loss_fn, config['device'])\n        loss_validation, accuracy_validation = validation_step(model, validation_loader, loss_fn, config['device'])\n        loss_test, accuracy_test = test_step(model, test_loader, loss_fn, config['device'])\n        \n        wandb.log({\n            \"loss_train\": loss_train,\n            \"loss_validation\": loss_validation,\n            \"accuracy_validation\": accuracy_validation,\n            \"loss_test\": loss_test,\n            \"accuracy_test\": accuracy_test\n        })\n        \n    # Push model to the Hub\n    model.push_to_hub(\"Fatou/asr2bert-sentimentanalysis\")\n    tokenizer.push_to_hub(\"Fatou/asr2bert-sentimentanalysis\")\n    model.pretrained_model.config.push_to_hub(\"Fatou/asr2bert-sentimentanalysis\")\n\n    # Affichage des variables enregistrées dans wandb.log à la fin de l'exécution\n    wandb.run.finish()  # Termine l'exécution WandB\n    history = wandb.run.history()\n        \n        \n    \n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-16T09:37:37.414211Z","iopub.execute_input":"2023-10-16T09:37:37.414590Z","iopub.status.idle":"2023-10-16T09:37:37.422765Z","shell.execute_reply.started":"2023-10-16T09:37:37.414562Z","shell.execute_reply":"2023-10-16T09:37:37.421896Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-10-16T09:37:45.114271Z","iopub.execute_input":"2023-10-16T09:37:45.115508Z","iopub.status.idle":"2023-10-16T12:01:24.698958Z","shell.execute_reply.started":"2023-10-16T09:37:45.115469Z","shell.execute_reply":"2023-10-16T12:01:24.697555Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ece31193320422495b987e932791644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c15d3adb438649868bbed5490f6364bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ffd9609422d48beb24e2f2b7ca2c613"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc56986cd1c24f81b9429d735b7f67e9"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231016_093928-oduqt3f6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ndeyefatousene1502/asr-bert-sentiment-analysis/runs/oduqt3f6' target=\"_blank\">cerulean-haze-6</a></strong> to <a href='https://wandb.ai/ndeyefatousene1502/asr-bert-sentiment-analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ndeyefatousene1502/asr-bert-sentiment-analysis' target=\"_blank\">https://wandb.ai/ndeyefatousene1502/asr-bert-sentiment-analysis</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ndeyefatousene1502/asr-bert-sentiment-analysis/runs/oduqt3f6' target=\"_blank\">https://wandb.ai/ndeyefatousene1502/asr-bert-sentiment-analysis/runs/oduqt3f6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80052f5de42144e2bae97e1f1357ee1c"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 16000/16000 [32:27<00:00,  8.22it/s]\n100%|██████████| 2000/2000 [01:13<00:00, 27.03it/s]\n100%|██████████| 2000/2000 [01:12<00:00, 27.46it/s]\n 82%|████████▏ | 13108/16000 [26:47<05:52,  8.21it/s]IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n100%|██████████| 16000/16000 [32:42<00:00,  8.15it/s]\n100%|██████████| 2000/2000 [01:14<00:00, 26.87it/s]\n100%|██████████| 2000/2000 [01:13<00:00, 27.35it/s]\n100%|██████████| 16000/16000 [32:55<00:00,  8.10it/s]\n100%|██████████| 2000/2000 [01:17<00:00, 25.88it/s]\n100%|██████████| 2000/2000 [01:16<00:00, 26.31it/s]\n100%|██████████| 16000/16000 [32:30<00:00,  8.20it/s]\n100%|██████████| 2000/2000 [01:14<00:00, 26.94it/s]\n100%|██████████| 2000/2000 [01:12<00:00, 27.43it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/670M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"529581ea197945739301f64eab31bd61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy_test</td><td>▄█▁▄</td></tr><tr><td>accuracy_validation</td><td>█▇▁█</td></tr><tr><td>loss_test</td><td>▂▁█▆</td></tr><tr><td>loss_train</td><td>█▅▃▁</td></tr><tr><td>loss_validation</td><td>█▁█▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy_test</td><td>54.355</td></tr><tr><td>accuracy_validation</td><td>54.415</td></tr><tr><td>loss_test</td><td>0.22209</td></tr><tr><td>loss_train</td><td>0.11426</td></tr><tr><td>loss_validation</td><td>0.21872</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cerulean-haze-6</strong> at: <a href='https://wandb.ai/ndeyefatousene1502/asr-bert-sentiment-analysis/runs/oduqt3f6' target=\"_blank\">https://wandb.ai/ndeyefatousene1502/asr-bert-sentiment-analysis/runs/oduqt3f6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231016_093928-oduqt3f6/logs</code>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[7], line 51\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Affichage des variables enregistrées dans wandb.log à la fin de l'exécution\u001b[39;00m\n\u001b[1;32m     50\u001b[0m wandb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mfinish()  \u001b[38;5;66;03m# Termine l'exécution WandB\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m()\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'history'"],"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'history'","output_type":"error"}]}]}